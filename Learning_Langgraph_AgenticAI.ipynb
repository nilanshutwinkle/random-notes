{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQYL0q85Ua9RbEMhVgnFps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilanshutwinkle/random-notes/blob/master/Learning_Langgraph_AgenticAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LEARNING LANGGRAPH\n",
        "---\n",
        "Calling Tools"
      ],
      "metadata": {
        "id": "qxvc6fakHX3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_openai langchain-experimental langchain-community openai langchain-core langgraph --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts2q4ZBXHse9",
        "outputId": "e68dcc3b-ea60-45a8-dde3-9e76614d3c6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, re, json, asyncio\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4McPJXTSHdLv",
        "outputId": "edad04e1-b91d-472a-db4c-e0dc8651b008"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model = 'gpt-4o-mini', api_key = userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "Xs_MLyibXmFE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(state: MessagesState):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  print(response)\n",
        "  return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "FobXqpIcX8A6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "workflow.add_node('call_llm', call_llm)\n",
        "\n",
        "workflow.add_edge(START, 'call_llm')\n",
        "workflow.add_edge('call_llm', END)\n",
        "\n",
        "app = workflow.compile(checkpointer=MemorySaver())"
      ],
      "metadata": {
        "id": "5iDnYov6YdEM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interact_with_agent_with_true_session():\n",
        "  while True:\n",
        "    thread_id = input(\"Enter threadId, or type 'new' to start with a fresh one\")\n",
        "    if thread_id.lower() in ['quit', 'exit', 'bye']:\n",
        "      print(\"Ending conversation ... \")\n",
        "      break\n",
        "\n",
        "    if thread_id.lower() == 'new':\n",
        "      thread_id = f\"session_{os.urandom(4).hex()}\"\n",
        "\n",
        "    while True:\n",
        "      user_input = input(\"You: \")\n",
        "      if user_input.lower() in ['quit', 'bye', 'exit']:\n",
        "        print(f'Ending Session {thread_id}')\n",
        "        break\n",
        "\n",
        "      input_message = {\n",
        "      \"messages\": [(\"human\", user_input)]\n",
        "      }\n",
        "      # Invoke the graph with the correct thread ID to maintain memory acrosssessions\n",
        "      config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "      for chunk in app.stream(input_message, config=config, stream_mode=\"values\"):\n",
        "        chunk[\"messages\"][-1].pretty_print()\n",
        "\n",
        "interact_with_agent_with_true_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4C4_2OkY3lV",
        "outputId": "bacc6a35-2e23-450e-abd3-1a51a5783295"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter threadId, or type 'new' to start with a fresh onenew\n",
            "You: input_message = { \"messages\": [(\"human\", user_input)] } # Invoke the graph with the correct thread ID to maintain memory across sessions config = {\"configurable\": {\"thread_id\": thread_id}} for chunk in app_with_memory.stream(input_message, config=config, stream_mode=\"values\"): chunk[\"messages\"][-1].pretty_print()\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "input_message = { \"messages\": [(\"human\", user_input)] } # Invoke the graph with the correct thread ID to maintain memory across sessions config = {\"configurable\": {\"thread_id\": thread_id}} for chunk in app_with_memory.stream(input_message, config=config, stream_mode=\"values\"): chunk[\"messages\"][-1].pretty_print()\n",
            "content='It looks like you are trying to create a system that utilizes a chat interface where messages are sent and responses are streamed back, with the use of some sort of memory configuration. Your input message contains a human input, and you are passing this along with a thread ID to maintain the context of the conversation across multiple sessions.\\n\\nHere\\'s a brief breakdown of the code snippet:\\n\\n1. **Input Structure**: \\n   - You create an `input_message` dictionary that includes a list of messages. The list contains a tuple with the first element as `\"human\"` (indicating the source of the message) and the second element being `user_input` (the actual message).\\n\\n2. **Configuration**:\\n   - A `config` dictionary is defined that contains a `\"configurable\"` key where you pass the `thread_id`. This is presumably for context or state management across interactions.\\n\\n3. **Streaming**:\\n   - The `app_with_memory.stream()` function is invoked with the `input_message` and `config`, specifying `stream_mode` as `\"values\"`. This suggests that the output will be streamed data values, likely one at a time or in chunks.\\n\\n4. **Output Handling**:\\n   - Inside the streaming loop, each chunk (which seemingly contains a list of messages) is processed, and the last message in the chunk is printed in a pretty format using `pretty_print()`.\\n\\nIf you need further assistance or have any specific questions regarding this implementation or would like to enhance certain aspects, feel free to ask!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 77, 'total_tokens': 384, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-Bfs0Fgx6CyhhZtdCKmj2AG1ZePns0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--30797551-c274-4cee-a93c-5209d36faecf-0' usage_metadata={'input_tokens': 77, 'output_tokens': 307, 'total_tokens': 384, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "It looks like you are trying to create a system that utilizes a chat interface where messages are sent and responses are streamed back, with the use of some sort of memory configuration. Your input message contains a human input, and you are passing this along with a thread ID to maintain the context of the conversation across multiple sessions.\n",
            "\n",
            "Here's a brief breakdown of the code snippet:\n",
            "\n",
            "1. **Input Structure**: \n",
            "   - You create an `input_message` dictionary that includes a list of messages. The list contains a tuple with the first element as `\"human\"` (indicating the source of the message) and the second element being `user_input` (the actual message).\n",
            "\n",
            "2. **Configuration**:\n",
            "   - A `config` dictionary is defined that contains a `\"configurable\"` key where you pass the `thread_id`. This is presumably for context or state management across interactions.\n",
            "\n",
            "3. **Streaming**:\n",
            "   - The `app_with_memory.stream()` function is invoked with the `input_message` and `config`, specifying `stream_mode` as `\"values\"`. This suggests that the output will be streamed data values, likely one at a time or in chunks.\n",
            "\n",
            "4. **Output Handling**:\n",
            "   - Inside the streaming loop, each chunk (which seemingly contains a list of messages) is processed, and the last message in the chunk is printed in a pretty format using `pretty_print()`.\n",
            "\n",
            "If you need further assistance or have any specific questions regarding this implementation or would like to enhance certain aspects, feel free to ask!\n",
            "You: What is the capital of Assam?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the capital of Assam?\n",
            "content='The capital of Assam is Dispur. Dispur is located near Guwahati, which is one of the largest cities in Assam.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 399, 'total_tokens': 426, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bfs0XwdELBcGJFRsjSkIQ4fPbsS96', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7ca05a68-9f24-4488-b59f-e544252adeff-0' usage_metadata={'input_tokens': 399, 'output_tokens': 27, 'total_tokens': 426, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The capital of Assam is Dispur. Dispur is located near Guwahati, which is one of the largest cities in Assam.\n",
            "You: bye\n",
            "Ending Session session_52b89b78\n",
            "Enter threadId, or type 'new' to start with a fresh onesession_52b89b78\n",
            "You: which state I mentioned earlier\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "which state I mentioned earlier\n",
            "content='You mentioned Assam earlier.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 439, 'total_tokens': 444, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bfs183hrLvewhGRRWpwEKPS1dOy9n', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e5dfbde5-ff57-44de-beec-057c937f4d03-0' usage_metadata={'input_tokens': 439, 'output_tokens': 5, 'total_tokens': 444, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You mentioned Assam earlier.\n",
            "You: bye\n",
            "Ending Session session_52b89b78\n",
            "Enter threadId, or type 'new' to start with a fresh onebye\n",
            "Ending conversation ... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In Memory Store\n",
        "---\n",
        "Memory Store allows the agent to store information that can be shared\n",
        "across different sessions (threads) for the same user."
      ],
      "metadata": {
        "id": "mNyaSxnkkMyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.store.memory import InMemoryStore\n",
        "import uuid\n",
        "\n",
        "in_memory_store = InMemoryStore()\n",
        "def store_user_info(state: MessagesState, config, *, memory_store = in_memory_store):\n",
        "  user_id = config[\"configurable\"][\"user_id\"]\n",
        "  namespace = (user_id, \"memories\")\n",
        "\n",
        "  # Create a memory based on conversation\n",
        "  memory_id = str(uuid.uuid4())\n",
        "  memory = {\"user_name\": state[\"user_name\"]}\n",
        "\n",
        "  # Save the memory to in-memory store\n",
        "  memory_store.put(namespace, memory_id, memory)\n",
        "  return {\"message\": [\"User information saved.\"]}\n",
        "\n",
        "# Function to retrive stored user information\n",
        "def retrive_user_info(state: MessagesState, config, *, memory_store = in_memory_store):\n",
        "  user_id = config[\"configurable\"][\"user_id\"]\n",
        "  namespace = (user_id, \"memories\")\n",
        "\n",
        "  # Retrieve the stored memory\n",
        "  memories = memory_store.search(namespace)\n",
        "  if memories:\n",
        "    info = f'Hello {memories[-1].value[\"user_name\"]}, welcome back!'\n",
        "  else:\n",
        "    info = f'No user information found'\n",
        "\n",
        "  return {\"message\": [info]}"
      ],
      "metadata": {
        "id": "MEiDHp3Yf4OX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete code"
      ],
      "metadata": {
        "id": "G4t3j_3-oF7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, re, json, asyncio\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1680-11woFkz",
        "outputId": "2946e6d7-e904-4fce-f7e0-44c596bb8ef6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model = 'gpt-4o-mini', api_key = userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "JILlxeFMnG6H"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_memory_store = InMemoryStore()"
      ],
      "metadata": {
        "id": "EXUUjJjoobiS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: MessagesState, config):\n",
        "  last_message = state[\"messages\"][-1].content.lower()\n",
        "  if 'remember my name is' in last_message:\n",
        "    user_name = last_message[-1].split('remember my name is')[-1].strip()\n",
        "    state['user_name'] = user_name\n",
        "    return store_user_info(state, config)\n",
        "\n",
        "  if 'what is my name' in last_message:\n",
        "    return retrive_user_info(state, config)\n",
        "\n",
        "  return {\"messages\": [\"Sorry I didn't understand your request.\"]}"
      ],
      "metadata": {
        "id": "bND4yzZ9ogTp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "workflow.add_node('call_model', call_model)\n",
        "\n",
        "workflow.add_edge(START, 'call_model')\n",
        "workflow.add_edge('call_model', END)\n",
        "\n",
        "app = workflow.compile(checkpointer=MemorySaver(), store= in_memory_store)"
      ],
      "metadata": {
        "id": "VxiWs8eNp2rT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_session():\n",
        "  config = {\"configurable\" : {\"thread_id\": \"session_1\", \"user_id\": \"user_123\"}}\n",
        "\n",
        "  input_message = {\"messages\": [{\"type\": \"user\", \"content\": \"Remember my name is Bruno\"}]}\n",
        "  for chunk in app.stream(input_message, config=config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()\n",
        "\n",
        "  config = {\"configurable\" : {\"thread_id\": \"session_2\", \"user_id\": \"user_123\"}}\n",
        "  input_message = {\"messages\": [{\"type\": \"user\", \"content\": \"What is my name\"}]}\n",
        "  for chunk in app.stream(input_message, config=config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()\n",
        "\n",
        "simulate_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBGW6NYaqD7y",
        "outputId": "76a6d8cc-62d2-41ae-e984-b088d67c92c7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Remember my name is Bruno\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is my name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CRr5FLuqsYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}